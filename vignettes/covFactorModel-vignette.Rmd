---
title: "Estimation of covariance matrix via factor models"
author: "Rui Zhou and Daniel P. Palomar"
date: "`r Sys.Date()`"
output:
  bookdown::html_document2:  
    base_format: prettydoc::html_pretty
    theme: tactile
    highlight: vignette
    fig_caption: yes
    number_sections: no
    toc: yes
    toc_depth: 2
  bookdown::pdf_document2:
    fig_caption: yes
    number_sections: yes
    toc: yes
    toc_depth: 2
indent: yes
csl: ieee.csl
bibliography: refs.bib
vignette: |
  %\VignetteIndexEntry{Estimation of covariance matrix via factor models}
  %\VignetteKeyword{factor model, covariance matrix}
  %\VignetteEncoding{UTF-8} 
  %\VignetteEngine{knitr::rmarkdown}
---

```{r, echo = FALSE}
library(knitr)
options(width = 1000)  # output width
opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.align = "center",
  fig.retina = 2,
  out.width = "75%",
  dpi = 96
)
knit_hooks$set(pngquant = hook_pngquant)
#rmarkdown::render("vignettes/covFactorModel-vignette.Rmd", "all")
#rmarkdown::render("vignettes/covFactorModel-vignette.Rmd", "bookdown::html_document2")
#rmarkdown::render("vignettes/covFactorModel-vignette.Rmd", "bookdown::pdf_document2")
#tools::compactPDF("vignettes/covFactorModel-vignette.pdf", gs_quality = "ebook")
```
-----------
> This vignette illustrates the estimation of covariance matrix via factor models with the package `covFactorModel` and gives a description of the algorithms used.


# Comparison with other packages
We compare the provided package `covFactorModel` with the existing package `FinCovRegularization` and function `stats::factanal()`.
First, we compare the estimation results with package `FinCovRegularization`, which also estimates the covariance matrix of asset returns via three types of factor models as our package. We start by loading built-in data from package `FinCovRegularization`:
```{r, message = FALSE, warning = FALSE, cache = TRUE}
library(FinCovRegularization)
library(xts)

# load raw data
data(m.excess.c10sp9003)
assets <- m.excess.c10sp9003[, 1:10]
factor <- m.excess.c10sp9003[, 11]
T <- nrow(assets)
# convert data into xts object
assets_xts <- as.xts(assets, order.by = as.Date("1995-03-15")+1:T)
factor_xts <- as.xts(factor, order.by = as.Date("1995-03-15")+1:T)

# sector information for BARRA Industry factor model
# from help page of function FinCovRegularization::FundamentalFactor.Cov
beta <- matrix(0, 10, 3)
dimnames(beta) <- list(colnames(assets), c("Drug", "Auto", "Oil"))
beta[c("ABT", "LLY", "MRK", "PFE"), "Drug"] <- 1
beta[c("F", "GM"), "Auto"] <- 1
beta[c("BP", "CVX", "RD", "XOM"), "Oil"] <- 1
sector_info <- c(rep(1, 4),
                 rep(2, 2),
                 rep(3, 4))
```
Then, we use the two packages to compute the covariance matrix estimation (the comparison of execution time is ignored here because their implementation are almost same) via three factor models and compare the results:
```{r, message = FALSE, warning = FALSE, cache = TRUE}
library(covFactorModel)
# compare cov by macroeconomic factor model
my_cov_macro <- covFactorModel(assets_xts, type = "Macro", econ_fact = factor_xts)
cov_macro <- MacroFactor.Cov(assets, factor)
norm(cov_macro - my_cov_macro, "F")

# compare cov by BARRA Industry factor model
my_cov_BARRA <- covFactorModel(assets_xts, type = "Barra", stock_sector_info = sector_info)
cov_BARRA <- FundamentalFactor.Cov(assets, exposure = beta, method = "OLS")
norm(cov_BARRA - my_cov_BARRA, "F")

# compare cov by statistical factor model
my_cov_stat <- covFactorModel(assets_xts, type = "Stat-PCA", K = 3)
cov_stat <- StatFactor.Cov(assets, 3)
norm(cov_stat - my_cov_stat, "F")
```
It is clear that the covariance matrix estimation results from the packages `covFactorModel` and `FinCovRegularization` are exactly the same. Note that `covFactorModel` allows the user to choose different structures on residual covariance matrix (i.e., scaled identity, diagonal, block diagonal, and full), while `FinCovRegularization` assumes it to be diagonal only. (When use the BARRA Industry factor model, `covFactorModel` requires sector information in vector form or nothing if column names of data matrix is contained in the in-built database `data(stock_sector_database)`, while `FinCovRegularization` forces user to pass the sector information in matrix form.)

Next, we compare the performance of `covFactorModel()` and `stats::factanal()` in covariance matrix estimation. From the description of `factanal()` (use `?factanal` for details), it performs a maximum-likelihood factor analysis on a covariance matrix or data matrix and is in essence a model for the correlation matrix. We compare the correlation matrix estimation in terms of PRIAL (see next section for details) and running time. Since `covFactorModel()` returns the covariance matrix, we use `cov2cor()` to obtain the correlation matrix. As shown in Figure \@ref(fig:package-compare), `covFactorModel()` can achieve a similar estimation performance but is much faster compared with `factanal()`.
```{r package-compare, echo = FALSE, out.width = "100%", fig.cap = "Average PRIAL and running time."}
knitr::include_graphics("figures/package_compare.png", auto_pdf = TRUE)
```

Summarizing, our package `covFactorModel` performs the same as the package `FinCovRegularization` in terms of covariance matrix estimation and computational speed for the three cases of Macroeconomic factor model, BARRA Industry factor model, and statistical PCA factor model (although `covFactorModel` allows for more types of structure in the residual covariance matrix). In addtition, our package `covFactorModel` can implement the statistical ML factor model with the same performance as the function `stats::factanal()` but with a much faster computational speed (one order of magnitude faster).



# Usage of the package

## Usage of `factorModel()`
The function `factorModel()` builds a factor model for the data, i.e., it decomposes the asset returns into a factor component and a residual component. The user can choose different types of factor models, namely, macroeconomic, BARRA, or statistical. We start by loading some real market data using package `quantmod`:
```{r, message = FALSE, warning = FALSE, cache = TRUE}
library(xts)
library(quantmod)

# set begin-end date and stock namelist
begin_date <- "2016-01-01"
end_date <- "2017-12-31"
stock_namelist <- c("AAPL", "AMD", "ADI",  "ABBV", "AET", "A",  "APD", "AA","CF")

# download stock data from YahooFinance
data_set <- xts()
for (stock_index in 1:length(stock_namelist))
  data_set <- cbind(data_set, Ad(getSymbols(stock_namelist[stock_index], 
                                            from = begin_date, to = end_date, 
                                            auto.assign = FALSE)))
colnames(data_set) <- stock_namelist
indexClass(data_set) <- "Date"
# check stock data
head(data_set)
tail(data_set)

# download SP500 Index data from YahooFinance
SP500_index <- Ad(getSymbols("^GSPC", from = begin_date, to = end_date, auto.assign = FALSE))
colnames(SP500_index) <- "index"
# check SP500 index data
head(SP500_index)
```

We first build a _macroeconomic factor model_, where `SP500_index` is used as one macroeconomic factor:
```{r}
library(covFactorModel)
# compute log-returns
X <- diff(log(data_set), na.pad = FALSE)
f <- diff(log(SP500_index), na.pad = FALSE)
N <- ncol(X)  # number of stocks
T <- nrow(X)  # number of days

# use package to build macroeconomic factor model
macro_econ_model <- factorModel(X, type = "Macro", econ_fact = f)

# sanity check
X_ <- with(macro_econ_model, 
           matrix(alpha, T, N, byrow = TRUE) + f %*% t(beta) + residual)
norm(X - X_, "F")

par(mfrow = c(1, 2))
barplot(macro_econ_model$alpha, horiz = TRUE, 
        main = "alpha", col = "red", cex.names = 0.75, las = 1)
barplot(t(macro_econ_model$beta), horiz = TRUE, 
        main = "beta", col = "blue", cex.names = 0.75, las = 1)
```

In finance, this is also known as capital asset pricing model (CAPM) assuming the risk free rate is zero. The term `alpha` is the stock's abnormal return and `beta` is the stock's responsiveness to the market return.

Next, we build a _BARRA industry factor model:_
```{r}
barra_model <- factorModel(X, type = "Barra")
print(barra_model$beta)

# sanity check
X_ <- with(barra_model, 
           matrix(alpha, T, N, byrow = TRUE) + factors %*% t(beta) + residual)
norm(X - X_, "F")
```

Finally, we build a _statistical factor model_, which is based on principal component analysis (PCA):
```{r}
# set factor dimension as K=2
stat_model <- factorModel(X, K = 2)

# sanity check
X_ <- with(stat_model, 
           matrix(alpha, T, N, byrow = TRUE) + factors %*% t(beta) + residual)
norm(X - X_, "F")
```

## Usage of `covFactorModel()`
The function `covFactorModel()` estimates the covariance matrix of the data based on factor models. The user can choose not only the type of factor model (i.e., macroeconomic, BARRA, or statistical) but also the structure of the residual covariance matrix (i.e., scaled identity, diagonal, block diagonal, and full).

Firstly, we compare covariance matrix estimation based on different factor model decompositions. Let's start by preparing some parameters for the synthetic data generation:
```{r, fig.width = 3, out.width = "20%", fig.asp = 3}
library(covFactorModel)
library(xts)
library(MASS)
library(pheatmap)

# create parameters for generation of synthetic data
N <- 200  # number of stocks
mu <- rep(0, N) 
num_sectors <- 5 # num of sectors
stock_sector_info <- rep(1:num_sectors, each = N/num_sectors)
# generate beta following BARRA model
beta <- matrix(0, N, num_sectors)
for (i in 1:num_sectors) {
  mask <- stock_sector_info == i
  beta[mask, i] <- 1
}
# show beta
colnames(beta) <- paste0("f", 1:num_sectors)
pheatmap(beta, cluster_rows = FALSE, cluster_cols = FALSE, color = c(1, 0), legend = FALSE,
         main = "Factor loading (beta)")
```
```{r, fig.width = 7}
Psi <- diag(N)
Sigma_f <- diag(num_sectors)
Sigma <- beta %*% Sigma_f %*% t(beta) + Psi

# plot first 20 eigenvalues of Sigma
plot(eigen(Sigma)$values[1:20], type = "o", pch = 20, xlab = "eigenvalue index", ylab = "value")
```

Then, we simply use function `covFactorModel()` (by default it uses a diagonal structure for the residual covariance matrix). We show the square error (SE) $\lVert \hat{\boldsymbol{\Sigma}} - \boldsymbol{\Sigma}_{\mathsf{true}} \rVert _{F}^{2}$ w.r.t. number of observations:
```{r, fig.width = 7, fig.height = 5}
# generate synthetic data
set.seed(234)
err_scm <- err_macroecon <- err_barra <- err_statPCA <- err_statML <- c()
index_T <- N*seq(10)
for (T in index_T) {
  # generate factors and observed data matrix
  factors <- xts(mvrnorm(T, rep(0, num_sectors), Sigma_f), 
                 order.by = as.Date('1995-03-15') + 1:T)
  X <- factors %*% t(beta) + xts(mvrnorm(T, mu, Psi), order.by = index(factors))
   
  # sample covariance matrix
  err_scm <- c(err_scm, norm(Sigma - cov(X), "F")^2)
  
  # macroeconomic factor model
  cov_macroecon <- covFactorModel(X, type = "Macro", econ_fact = factors)
  err_macroecon <- c(err_macroecon, norm(Sigma - cov_macroecon, "F")^2)
  
  # BARRA factor model
  cov_barra <- covFactorModel(X, type = "Barra", stock_sector_info = stock_sector_info)
  err_barra <- c(err_barra, norm(Sigma - cov_barra, "F")^2)
  
  # statistical factor model by PCA with diagonal Psi (default)
  cov_statPCA <- covFactorModel(X, type = "Stat-PCA", K = num_sectors)
  err_statPCA <- c(err_statPCA, norm(Sigma - cov_statPCA, "F")^2)
  
  # statistical factor model by ML with diagonal Psi (default)
  cov_statML <- covFactorModel(X, type = "Stat-ML", K = num_sectors)
  err_statML <- c(err_statML, norm(Sigma - cov_statML, "F")^2)
}
res <- cbind("SCM"               = err_scm,
             "macroeconomic"     = err_macroecon,
             "BARRA"             = err_barra,
             "statistical-PCA"   = err_statPCA,
             "statistical-ML"    = err_statML)
colors <- c("blue", "green4", "darkmagenta", "red3", "gray0")
matplot(index_T/N, res,
        xlab = "T/N", ylab = "SE",
        main = "SE using different factor models",
        type = "b", pch = 20, lwd = 2, col = colors)
legend("topright", inset = 0.01, legend = colnames(res), pch = 20, col = colors)
```

Obviously, using factor models for covariance matrix estimation definitely helps (note that BARRA is obviously the best simply because the synthetic data was generated according to the BARRA model). In order to show how well the estimated covariance matrices do compared to the sample covariance matrix (benchmark), the estimation error can also be evaluated in terms of PRIAL (PeRcentage Improvement in Average Loss):
$$\mathsf{PRIAL}=100\times\frac{\lVert\hat{\mathbf{S}}-\boldsymbol{\Sigma}_{\mathsf{true}}\rVert_{F}^{2}-\lVert\hat{\boldsymbol{\Sigma}}-\boldsymbol{\Sigma}_{\mathsf{true}}\rVert_{F}^{2}}{\lVert\hat{\mathbf{S}}-\boldsymbol{\Sigma}_{\mathsf{true}}\rVert_{F}^{2}}$$
which goes to 0 when the estimation $\hat{\boldsymbol{\Sigma}}$ tends to the sample covariance matrix $\hat{\mathbf{S}}$ and goes to 100 when the estimation $\hat{\boldsymbol{\Sigma}}$ tends to the true covariance matrix $\boldsymbol{\Sigma}_{\sf true}$.

```{r, fig.width = 7, fig.height = 5}
PRIAL <- 100*(1 - apply(res, 2, "/", res[, 1]))
matplot(index_T/N, PRIAL,
        xlab = "T/N", ylab = "PRIAL",
        main = "PRIAL using different factor model",
        type = "b", pch = 20, lwd = 2, col = colors)
legend("topright", inset=0.02, legend = colnames(res), pch = 20, col = colors)
```

The performance of BARRA Industry and macroeconomic factor models seems better than that of the statistical factor model, but this is just because the synthetic data has been generated according to the BARRA model and because the macroeconomic factor model has been fed with the exact factors. The reality of market data may be different with other results (e.g., the industry information might be missing or wrong because it changes over time, and so are the factors). The statistical factor model is always easier to implement and more robust to the aforementioned practical issues.

In Figure \@ref(fig:cov-stat-perform), we generate synthetic data using $\boldsymbol{\Psi}$ with different structures, namely, diagonal, block diagonal, scaled identity, and full. Then we estimate the covariance matrix using the statistical factor model (imposing different structures on $\boldsymbol{\Psi}$) and show the performance. The estimation based on the statistical factor model can beat the sample covariance matrix mostly except when $\boldsymbol{\Psi}$ has a full structure (i.e., no structure at all).

```{r cov-stat-perform, echo = FALSE, out.width = "100%", fig.cap = "Performance of estimation under different Psi structures."}
knitr::include_graphics("figures/cov_stat_perform.png", auto_pdf = TRUE)
```

\newpage
## Usage of `getSectorInfo()`
The function `getSectorInfo()` provides sector information for a given set of stock symbols. The usage is very simple:
```{r}
library(covFactorModel)

mystocks <- c("AAPL",  "ABBV", "AET", "AMD", "APD", "AA","CF", "A", "ADI", "IBM")
getSectorInfo(mystocks)
```

The built-in sector database can be overidden by providing a stock-sector pairing:
```{r}
my_stock_sector_database <- cbind(mystocks, c(rep("sector1", 3),
                                              rep("sector2", 4),
                                              rep("sector3", 3)))
getSectorInfo(mystocks, my_stock_sector_database)
```


# Explanation of the algorithms

The factor model decomposes the stock returns into two parts: low-dimensional factors and idiosyncratic residual noise. There are three basic types of factor models [@tsay2005analysis], namely, macroeconomic, fundamental, and statistical. Suppose there are $N$ stocks in market and we have $T$ observations, then factor models can be expressed in linear form:
$$x_{i,t} = \alpha_{i} + \beta_{1,i}f_{1,t} + \dots + \beta_{K,i}f_{K,t} + \epsilon_{i,t}, \; t = 1, \dots, T$$
where $i$ is the stock index, $K$ is the number of factors, $\alpha_{i}$ is the intercept of the $i$-th stock, $\mathbf{f}_{k} = [f_{k,1}, \dots, f_{k,T}]^{T}$ is the common $k$-th factor, $\boldsymbol{\beta}_{i} = [\beta_{1,i}, \dots, \beta_{K,i}]^{T}$ is the factor loading of the $i$-th stock and $\epsilon_{i,t}$ is residual term for the $i$-th stock at time $t$. With the compact notation $\mathbf{F}=\left[\begin{array}{ccc} \mathbf{f}_{1} & \cdots & \mathbf{f}_{K}\end{array}\right]$, $\mathbf{x}_{i} = [x_{i,1}, \dots, x_{i,T}]^T$, and $\boldsymbol{\epsilon}_{i} = [\epsilon_{i,1}, \dots, \epsilon_{i,T}]^T$ it can also be written into vector form:
$$\mathbf{x}_{i} = \mathbf{1}\alpha_{i} + \mathbf{F} \boldsymbol{\beta}_{i} + \boldsymbol{\epsilon}_{i}, \; i=1,\dots,N$$

## `factorModel()`: Build factor model for given data

The goal of `factorModel()` is the decomposition of a $T\times N$ data matrix $\mathbf{X}$ into factors and residual idiosyncratic component. User can choose different types of factor models, namely, macroeconomic, BARRA (a special case of fundamental factor model), or statistical.

### Macroeconomic factor model (aka explicit factor model)
In this model, the factors are observed economic/financial time series. The macroeconomic factor model can be estimated through Least-Squares (LS) regression:
$$\underset{\boldsymbol{\gamma}_{i}}{\mathsf{minimize}}\quad\Vert\mathbf{x}_{i}-\tilde{\mathbf{F}}\boldsymbol{\gamma}_{i}\Vert^{2}$$
where $\tilde{\mathbf{F}}=\left[\begin{array}{cc} \mathbf{1}_{T} & \mathbf{F}\end{array}\right]$ and $\boldsymbol{\gamma}_{i}=\left[\begin{array}{c} \alpha_{i}\\ \boldsymbol{\beta}_{i} \end{array}\right]$. The closed-form solution is: $\hat{\boldsymbol{\gamma}}_{i}=\left(\tilde{\mathbf{F}}^{T}\tilde{\mathbf{F}}\right)^{-1}\tilde{\mathbf{F}}^{T}\mathbf{x}_{i}$. Then simply use the factor model decomposition to get the residual $\boldsymbol{\epsilon}_{i}=\mathbf{x}_{i}-\tilde{\mathbf{F}}\hat{\boldsymbol{\gamma}}_{i}$.

### BARRA Industry factor model (specific case of fundamental factor models)
Normally, fundamental factor model use observable asset specific characteristics (fundamentals) like industry classification, market capitalization, style classification (value, growth), etc., to determine the common risk factors $\mathbf{F}$. In this function, we only consider one of the cases: BARRA Industry factor model, which assumes that there are $K$ factors corresponding to $K$ mutually exclusive industries (aka, sectors). Apart from that, the loadings $\beta_{i,k}$ are directly defined as
$$\beta_{i,k}=\begin{cases}
1 & \textrm{if stock } i \textrm{ is in industry } k\\
0 & \textrm{otherwise.}
\end{cases}$$
Using compact combination $\mathbf{B}=\left[\begin{array}{ccc} \boldsymbol{\beta}_{1} & \cdots & \boldsymbol{\beta}_{N}\end{array}\right]^{T}$, the industry factor model is (note that $\boldsymbol{\alpha} = \mathbf{0}$):
$$\mathbf{x}_{t} =  \mathbf{B} \mathbf{f}_{t} + \boldsymbol{\epsilon}_{t}, \; t=1,\dots,T$$
where $\mathbf{x}_{t} = [x_{1,t},\dots,x_{N,t}]^T$ and $\mathbf{f}_{t} = [f_{1,t},\dots,f_{K,t}]^T$. Here the LS regression can also be applied to recover the factors (instead of the loadings as before) as
$$\underset{\mathbf{f}_{t}}{\mathsf{minimize}}\quad\frac{1}{T}\sum_{t=1}^{T}\Vert\mathbf{x}_{t}-\mathbf{B}\mathbf{f}_{t}\Vert_{2}^{2}$$
The solution is $\hat{\mathbf{f}}_{t}=(\mathbf{B}^{T}\mathbf{B})^{-1}\mathbf{B}^{T}\mathbf{x}_{t}, \; t=1,\dots,T$ and the residual can be simply calculated as $[\hat{\epsilon}_{1,t},\dots,\hat{\epsilon}_{N,t}]^{T}=\mathbf{x}_{t}-\mathbf{B}\hat{\mathbf{f}}_{t}$.

### Statistical factor model via PCA (aka implicit factor model)
 The statistical factor model via Principal Component Analysis (PCA) assumes that $\mathbf{f}_{t}$ is an affine transformation of $\mathbf{x}_{t}$, i.e., $\mathbf{f}_{t}=\mathbf{d}+\mathbf{C}^{T}\mathbf{x}_{t}$, where $\mathbf{d}\in\mathbb{R}^{K}$ and $\mathbf{C}\in\mathbb{R}^{N\times K}$ are parameters to be estimated. Therefore, we can formulate the problem as
$$
\underset{\boldsymbol{\alpha},\mathbf{B},\mathbf{C},\mathbf{d}}{\textrm{minimize}}\quad\frac{1}{T}\sum_{t=1}^{T}\Vert\mathbf{x}_{t}-\boldsymbol{\alpha}-\mathbf{B}\left(\mathbf{C}^{T}\mathbf{x}_{t}+\mathbf{d}\right)\Vert_{2}^{2}
$$
which is proven equivalent to PCA and thus solved by
$$
\hat{\boldsymbol{\alpha}}=\frac{1}{T}\sum_{t=1}^{T}\mathbf{x}_{t},\quad\hat{\mathbf{B}}=\hat{\mathbf{C}}=\hat{\boldsymbol{\Gamma}}^{(K)},\quad\hat{\mathbf{d}}=-\hat{\mathbf{C}}^{T}\hat{\boldsymbol{\alpha}}
$$
where $\hat{\boldsymbol{\Gamma}}^{(K)} \in \mathbb{R}^{N\times K}$ with $k$-th column being the $k$-th largest eigenvector of sample covariance matrix $\hat{\mathbf{S}}$. The desired structure on the residual covariance matrix is then enforced.

A further refinement of the above PCA method is based on an iterative approach where each iteration performs an improved PCA (the method above corresponds to just one iteration) [see [@MIT_opencourse_factormodel] for details]:

> **Algorithm 1**    
  1. Calculate sample covariance matrix $\hat{\mathbf{S}}$ and eigen-decomposition (EVD): $\hat{\boldsymbol{\Gamma}}_{1} \hat{\boldsymbol{\Lambda}}_{1} \hat{\boldsymbol{\Gamma}}^{T}_{1}$    
  2. Set index $s=1$   
  3. Compute $\hat{\mathbf{B}}_{(s)} = \hat{\boldsymbol{\Gamma}}_{(s)}^{(K)} \hat{\boldsymbol{\Lambda}}^{(K)\frac{1}{2}}_{(s)}$, $\hat{\boldsymbol{\Psi}}_{(s)} = \textrm{struct}(\hat{\boldsymbol{\Sigma}} - \hat{\mathbf{B}}_{(s)} \hat{\mathbf{B}}^{T}_{(s)})$, $\hat{\boldsymbol{\Sigma}}_{(s)} = \hat{\mathbf{B}}_{(s)} \hat{\mathbf{B}}^{T}_{(s)} + \hat{\boldsymbol{\Psi}}_{(s)}$      
  4. Update EVD: $\hat{\boldsymbol{\Sigma}} - \hat{\boldsymbol{\Psi}}_{(s)} = \hat{\boldsymbol{\Gamma}}_{(s+1)} \hat{\boldsymbol{\Lambda}}_{(s+1)} \hat{\boldsymbol{\Gamma}}^{T}_{(s+1)}$ and $s \gets s+1$   
  5. Repeat Steps 3-4 until convergence.   
  6. Return $(\hat{\mathbf{B}}_{(s)}, \hat{\boldsymbol{\Psi}}_{(s)}, \hat{\boldsymbol{\Sigma}}_{(s)})$   
        
where $\textrm{struct}()$ is to impose certain structure on $\hat{\boldsymbol{\Psi}}_{(s)}$, one typical option is diagonal. After the algorithm is done, we can calculate $\hat{\boldsymbol{\alpha}} = \frac{1}{T} \sum_{t=1}^{T} \mathbf{x}_{t}$ and build the statistical factor model using the algorithm output:
$$ \hat{\mathbf{B}} = \hat{\boldsymbol{\Gamma}}^{(K)} \hat{\boldsymbol{\Lambda}}^{(K)^{\frac{1}{2}}}, \quad \hat{\mathbf{f}}_{t} = \hat{\boldsymbol{\Lambda}}^{(K)^{-\frac{1}{2}}} \hat{\boldsymbol{\Gamma}}^{(K)^{T}} (\mathbf{x}_{t} - \hat{\boldsymbol{\alpha}}), \quad \hat{\boldsymbol{\epsilon}}_{t} = \mathbf{x}_{t} - \hat{\boldsymbol{\alpha}} - \hat{\mathbf{B}} \hat{\mathbf{f}}_{t}$$
(Note that the Algorithm 1 is equivalent to previous method when only one iteration is performed.)

## `covFactorModel()`: Covariance matrix estimation via factor models
### Through the factor decomposition obtained by the function `factorModel()`
The first approach is based on the factor model decomposition. As mentioned above, the factor model can be expressed as:
$$\mathbf{x}_{t} = \boldsymbol{\alpha} + \mathbf{B} \mathbf{f}_{t} + \boldsymbol{\epsilon}_{t},  \; t = 1, \dots, T$$
Assuming $\{\mathbf{f}_{t}\}$ and $\{\boldsymbol{\epsilon}_{t}\}$ are uncorrelated, the covariance matrix $\boldsymbol{\Sigma}$ can be written as
$$\boldsymbol{\Sigma} = \mathbf{B} \boldsymbol{\Sigma}_{\mathbf{f}} \mathbf{B}^{T} + \boldsymbol{\Psi}$$
where $\boldsymbol{\Sigma}_{\mathbf{f}} = \mathsf{Cov}[\mathbf{x}_{t}]$ and $\boldsymbol{\Psi} = \mathsf{Cov}[\boldsymbol{\epsilon}_{t}]$. Therefore, we can simply use result from function `factorModel()` to estimate covariance matrix $\boldsymbol{\Sigma}$ as:
$$\hat{\boldsymbol{\Sigma}} = \hat{\mathbf{B}} \hat{\boldsymbol{\Sigma}}_{\mathbf{f}} \hat{\mathbf{B}}^{T} + \hat{\boldsymbol{\Psi}}$$
where $\hat{\boldsymbol{\Sigma}}_{\mathbf{f}}$ and $\hat{\boldsymbol{\Psi}}$ are the sample covariance matrix of $\{\mathbf{\mathbf{f}}_{t}\}$ and $\{\boldsymbol{\epsilon}_{t}\}$. Besides,  the $\boldsymbol{\Psi}$ is expected to follow a special structure, i.e.,
$$\hat{\boldsymbol{\Sigma}} = \hat{\mathbf{B}} \hat{\boldsymbol{\Sigma}}_{\mathbf{f}} \hat{\mathbf{B}}^{T} + \textrm{struct}\{ \hat{\boldsymbol{\Psi}} \}.$$
In the statistical factor model by PCA of function `factorModel()`, the estimate $\hat{\boldsymbol{\Sigma}}$ is actually available when building the model. Therefore the algorithm output $\hat{\boldsymbol{\Sigma}}_{(s)}$ is directly extracted as the covariance matrix estimation.

### Through maximum likelihood (ML) estimation under the Gaussian assumption
Another popular statistical factor model covariance matrix estimation is based on maximum likelihood (ML) estimation. Under the Gaussian assumption on the returns, it can be formulated as
$$
\begin{aligned}\underset{\mathbf{B},\boldsymbol{\Psi}}{\mathsf{minimize}}\,\,\, & \,\,\,-\log\lvert\boldsymbol{\Sigma}^{-1}\rvert+\textrm{Tr}\left(\boldsymbol{\Sigma}^{-1}\hat{\mathbf{S}}\right)\\
\mathsf{subject\,\,to} & \,\,\,\boldsymbol{\Sigma}=\mathbf{B}\mathbf{B}^{T}+\boldsymbol{\Psi}\\
 & \,\,\,\mathbf{B}\in\mathbb{R}^{N\times K}\\
 & \,\,\,\boldsymbol{\Psi}=\textrm{diag}\left(\psi_{1},\dots,\psi_{p}\right)\ge\epsilon\mathbf{I}
\end{aligned}
$$
To solve this problem, we implement a very efficient algorithm from [@khamaru2018computation], which is much faster than the one used in `stats::factanal()`. We also give the complete procedure of the algorithm for reference, see Algorithm 2.

> **Algorithm 2**    
  1. Require $\boldsymbol{\Psi}^{\left(0\right)}$ and set $\boldsymbol{\Phi}^{\left(0\right)}=\left(\boldsymbol{\Psi}^{\left(0\right)}\right)^{-1}$, $t=0$    
  2. Repeat the following   
  3. Compute $\bigtriangledown_{i}=\left(\left(\boldsymbol{\Phi}^{\left(t\right)}\right)^{-\frac{1}{2}}\mathbf{U}\mathbf{D}_{1}\mathbf{U}^{T}\left(\boldsymbol{\Phi}^{\left(t\right)}\right)^{\frac{1}{2}}\mathbf{S}\right)_{ii}$ for $i=1,\dots,p$, where $\mathbf{U}\mathrm{diag}\left(\lambda_{1}^{*},\dots,\lambda_{p}^{*}\right)\mathbf{U}^{T}=\left(\boldsymbol{\Phi}^{\left(t\right)}\right)^{\frac{1}{2}}\mathbf{S}\left(\boldsymbol{\Phi}^{\left(t\right)}\right)^{\frac{1}{2}}$ and $\mathbf{D}_{1}=\mathrm{diag}\left(\delta_{1},\dots,\delta_{p}\right)$ with $\delta_{i}=\begin{cases}
\max\left\{ 0,1-\frac{1}{\lambda_{i}^{*}}\right\}  & 1\le i\le r\\
0 & \textrm{otherwise}
\end{cases}$       
  4. Update $\Phi_{ii}^{\left(t+1\right)}=\min\left\{ \frac{1}{S_{ii}-\bigtriangledown_{i}},\frac{1}{\epsilon}\right\}$  for $i=1,\dots,p$ and $t\leftarrow t+1$   
  5. Until convergence.   
  6. Recovery $\boldsymbol{\Psi}^{\star}=\left(\boldsymbol{\Phi}^{\left(t\right)}\right)^{-1}$    
  7. Compute $\mathbf{B}^{\star}=\left(\boldsymbol{\Psi}^{\star}\right)^{\frac{1}{2}}\left[\mathbf{z}_{1},\mathbf{z}_{2},\dots,\mathbf{z}_{r}\right]$ where $\mathbf{z}_{i}$ are the largest $r$ eigenvectors of $\left(\boldsymbol{\Psi}^{\star}\right)^{-\frac{1}{2}}\mathbf{S}\left(\boldsymbol{\Psi}^{\star}\right)^{-\frac{1}{2}}$ rescaled by corresponding largest $r$ eigenvalues $\lambda_{i}^{*},i\le r$, with $\lVert\mathbf{z}_{i}\rVert^{2}=\max\left\{ 1,\lambda_{i}^{*}\right\} -1$
 
# Macroeconomic factor model with sparse loading matrix
We impose no structure constraint on loading matrix $\mathbf{B}$ till now, which would weaken its interpretability or  incur the over-fitting problem. One convenient approach is to assume $\mathbf{B}$ is sparse, i.e., add regularization term to loss function. Recall the macroeconomic factor model estimation problem in compact form is
$$
\begin{aligned}\underset{\boldsymbol{\alpha},\mathbf{B}}{\mathsf{minimize}} & \quad\frac{1}{2T}\lVert\mathbf{X}-\mathbf{1}\boldsymbol{\alpha}^{T}-\mathbf{F}\mathbf{B}^{T}\rVert_{F}^{2}\end{aligned}
$$
In this Section, we show how to estimate loading matrix $\mathbf{B}$ of macroeconomic factor model with sparsity assumption. 

## Element-wise sparse
One of the most popular and convenient method is to add penalty element-wisely to $\mathbf{B}$, i.e.,
$$
\begin{aligned}\underset{\boldsymbol{\alpha},\mathbf{B}}{\mathsf{minimize}} & \quad\frac{1}{2T}\lVert\mathbf{X}-\mathbf{1}\boldsymbol{\alpha}^{T}-\mathbf{F}\mathbf{B}^{T}\rVert_{F}^{2}\end{aligned} +  \sum_{(i,j)} p \left( B_{i,j} \right)
$$
where $p(\theta)$ is a penalty function with some typical choices, e.g., minimax concave penalty (MCP), smoothly clipped absolute deviation (SCAD) and lasso. The three penalty functions are defined by:
$$p_{MCP}\left(\theta\right)=\begin{cases}
\lambda\lvert\theta\rvert-\frac{\theta^{2}}{2\gamma} & \lvert\theta\rvert\le\gamma\lambda\\
\frac{1}{2}\gamma\lambda^{2} & \lvert\theta\rvert\le\gamma\lambda
\end{cases}
\quad
P_{SCAD}\left(\theta\right)=\begin{cases}
\lambda\lvert\theta\rvert & \lvert\theta\rvert\le\lambda\\
\frac{\gamma\lambda\lvert\theta\rvert-0.5\left(\theta^{2}+\lambda^{2}\right)}{\gamma-1} & \lambda<\lvert\theta\rvert\le\gamma\lambda\\
\frac{\lambda^{2}\left(\gamma^{2}-1\right)}{2\left(\gamma-1\right)} & \lvert\theta\rvert>\gamma\lambda
\end{cases}
\quad
p_{lasso}\left(\theta\right)=\lambda\lvert\theta\rvert
$$

As each row of $\mathbf{B}$ is decoupled, the above problem can be decomposed into $N$ sub-problems with each be follows
$$
\begin{aligned}\underset{\alpha_{i},\boldsymbol{\beta}_{i}}{\mathsf{minimize}} & \quad\frac{1}{2T}\lVert\mathbf{x}_{i}-\mathbf{1}\alpha_{i}-\mathbf{F}\boldsymbol{\beta}_{i}\rVert_{2}^{2}\end{aligned} + \sum_{j}p\left(B_{i,j}\right)
$$

We found a package `ncvreg` which is very powerful to solve above problem with three mentioned penalty function. For matrix case, we can simply call it $N$ times and combine the final results. The example codes are given below as
```{r, message = FALSE, warning = FALSE}
library(ncvreg)
library(MASS)
set.seed(123)

# implement the function using package ncvreg
linreg_ele_sparse <- function(Factor, X, penalty = "lasso", lambda = 1, gamma = 4) {
  N <- ncol(X)
  K <- ncol(Factor)
  B <- matrix(0, N, K)
  alpha <- matrix(0, N, 1)
  for (i in 1:N) {
    tmp <- ncvreg(Factor, X[, i], family = "gaussian", 
                  penalty = penalty, lambda = lambda, gamma = gamma)$beta
    B[i, ] <- as.vector(tmp[-1])
    alpha[i] <- tmp[1]
  }
  return(list(
    "B" = B,
    "alpha" = alpha
  ))
}

# compare the real B with estimated version by three type of penalty function
# generate factor structure data with element-sparse B
T <- 50
N <- 10
K <- 3
lambda <- 0.01
Factor <- matrix(rnorm(T*K), T, K)
Factor <- mvrnorm(n = T, mu = rep(0, K), Sigma = diag(K))
B_real <- matrix(rnorm(K*N), N, K)
B_real[abs(B_real) < 0.5] <- 0
X <- Factor %*% t(B_real) + 0.01 * matrix(rnorm(T*N), T, N)

```
Let's do the least-square estimation without penalty term and see the results
```{r, message = FALSE, warning = FALSE}
fit_nopenalty <- linreg_ele_sparse(Factor, X, penalty = "lasso", lambda = 0)

# show real B and its non-penalty estimation
print(B_real)
print(fit_nopenalty$B)
```
It is significant that all the elements in estimated $\mathbf{B}$ are nonzero. Now let us estimate it again with penalty
```{r, message = FALSE, warning = FALSE}
fit_lasso <- linreg_ele_sparse(Factor, X, penalty = "lasso", lambda = 0.01)
fit_MCP <-   linreg_ele_sparse(Factor, X, penalty = "MCP", lambda = 0.01, gamma = 3)
fit_SCAD <-  linreg_ele_sparse(Factor, X, penalty = "SCAD", lambda = 0.01, gamma = 3.7)

# show result
print(fit_lasso$B)
print(fit_MCP$B)
print(fit_SCAD$B)
```
Obviously, the three estimations all give us a element-wise sparse estimation to $\mathbf{B}$.

## Column-wise sparsity
Sometimes, we may want the $\mathbf{B}$ be not only element-wise sparse, but also column-wise sparse. As the macroeconomic factors can be collected from many different sources like some public national statistical data or bought from the agencies, we hope some useless or low-impact factors could be identified via the building of factor model. If a column of estimated $\mathbf{B}$ is all zero, then the corresponding factor can be intuitively seen as useless or low-impact. We introduce here two methods to help get a column-wise sparse estimation for $\mathbf{B}$.

### Group Lasso
Basically, the group lasso problem is
$$
\begin{aligned}\underset{\boldsymbol{\beta}}{\textrm{minimize}} & \quad\frac{1}{2}\Bigg\lVert\mathbf{x}-\sum_{l}^{m}\mathbf{F}^{\left(l\right)}\boldsymbol{\beta}^{\left(l\right)}\Bigg\rVert+\lambda\sum_{l}^{m}\sqrt{p_{l}}\big\lVert\boldsymbol{\beta}^{\left(l\right)}\big\rVert_{2}\end{aligned}
$$
where $\mathbf{F}^{\left(l\right)}$ is the submatrix of $\mathbf{F}$ with columns corresponding to the factors in group $l$, $\boldsymbol{\beta}^{\left(l\right)}$ the coefficient vector of that group and $p_{l}$ is the length of $\boldsymbol{\beta}^{\left(l\right)}$. We can reformulate our macroeconomic factor model estimation problem by using group lasso
$$
\begin{aligned}\underset{\textrm{vec}\left(\mathbf{B}^{T}\right)}{\textrm{minimize}} & \quad\frac{1}{2}\Bigg\lVert\textrm{vec}\left(\mathbf{X}\right)-\left(\mathbf{I}\otimes\mathbf{F}\right)\textrm{vec}\left(\mathbf{B}^{T}\right)\Bigg\rVert+\lambda\sum_{l}^{m}\sqrt{p_{l}}\big\lVert\textrm{vec}\left(\mathbf{B}^{T}\right)^{\left(l\right)}\big\rVert_{2}\end{aligned}
$$
As our expectation, that $\mathbf{B}$ should be column-wise sparse, we cam simply pass the information that $i$-th and $\left(i+K\right)$-th factors are of same group. We found a package `SGL` which fits a linear regression model of lasso and group lasso regression, i.e.,
$$
\begin{aligned}\underset{\textrm{vec}\left(\mathbf{B}^{T}\right)}{\textrm{minimize}} & \quad\frac{1}{2}\Bigg\lVert\textrm{vec}\left(\mathbf{X}\right)-\left(\mathbf{I}\otimes\mathbf{F}\right)\textrm{vec}\left(\mathbf{B}^{T}\right)\Bigg\rVert+\left(1-\alpha\right)\lambda\sum_{l}^{m}\sqrt{p_{l}}\big\lVert\textrm{vec}\left(\mathbf{B}^{T}\right)^{\left(l\right)}\big\rVert_{2}+\alpha\lambda\big\lVert\textrm{vec}\left(\mathbf{B}^{T}\right)^{\left(l\right)}\big\rVert_{1}\end{aligned}
$$
where $\alpha$ is the turning parameter for a convex combination of the lasso and group lasso penalties. In our case, we realize what we want by the following R codes
```{r}
set.seed(123)
library(SGL)

# implement the function with package SGL
linreg_row_sparse <- function(Factor, X, lambda = 0.01, alpha = 0.85) {
  N <- ncol(X)
  K <- ncol(Factor)
  index <- rep(1:K, N)
  data <- list(x = diag(N) %x% Factor, y = as.vector(X))
  beta <- SGL(data, index, type = "linear", lambdas = lambda / N, alpha = alpha,
              thresh = 1e-5, standardize = FALSE)$beta
  B <- t(matrix(beta, K, N, byrow = FALSE))
  return(B)
}
```

Then, we generate data with some factors only influencing one among $N$ stocks, i.e., these factors are low-impact.
```{r}
n_lowimp <- 4
F_ <- mvrnorm(n = T, mu = rep(0, n_lowimp+K), Sigma = diag(n_lowimp+K))

B_ <- cbind(B_real, matrix(0, N, n_lowimp))
for (i in 1:n_lowimp) {
  B_[i, K+i] <- 0.5 
}
print(B_)
X_ <- F_ %*% t(B_) + 0.01 * matrix(rnorm(T*N), T, N)
```
We then compare the differences between element-wise sparse and row-wise sparse regression.
```{r}
B_elesparse <- linreg_ele_sparse(F_, X_, penalty = "lasso", lambda = 0.3)$B
B_rowsparse <- linreg_row_sparse(F_, X_, lambda = 0.3, alpha = 0.2)

print(B_)
print(B_elesparse)
print(B_rowsparse)
```
Obviously, we can obtain the row-sparse $\mathbf{B}$ using sparse-group lasso by properly choosing penalty coefficient $\lambda$ and $\alpha$.

### Subset selection
In machine learning, there exists a classical method called subset selection. We found a function `best.r.sq()` from a recently released R package `mvabund`, which implements a forward selection in a multivariate linear model.
```{r}
library(mvabund)
best.r.sq( X_~F_ )
```
Then, we can perform the trivial linear factor model regression with chosen factors.

# References {-}
\setlength{\parindent}{-0.2in}
\setlength{\leftskip}{0.2in}
\setlength{\parskip}{8pt}
\noindent
